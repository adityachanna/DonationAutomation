{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport subprocess\n\ndef run(commands):\n    for command in commands:\n        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\n            for line in sp.stdout:\n                line = line.decode(\"utf-8\", errors = \"replace\")\n                if \"undefined reference\" in line:\n                    raise RuntimeError(\"Failed Processing.\")\n                print(line, flush = True, end = \"\")\n        pass\n    pass\npass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install langchain langchain-core langchain-community langchain-groq langchain-google-genai langchain-ollama faiss-cpu pypdf gradio python-dotenv tavily-python tiktoken uuid","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import uuid\nimport logging\nimport base64\nfrom dotenv import load_dotenv\nfrom typing import Annotated, Dict, Any, Optional, List, Sequence, Literal, TypedDict\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel as PydanticBaseModel, Field # Alias Pydantic BaseModel\n\n# --- LangChain Core Imports ---\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.schema import Document\n\n# --- Document Loaders and Splitters ---\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# --- Vector Stores and Embeddings ---\nfrom langchain_community.vectorstores import FAISS\n# Ensure you have Ollama running and the model pulled\n# ollama pull nomic-embed-text\nfrom langchain_ollama import OllamaEmbeddings # Correct import\n\n# --- LLMs ---\n# Ensure you have GROQ and Google API keys set in your environment\nfrom langchain_groq import ChatGroq\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# --- Tools ---\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain.tools.retriever import create_retriever_tool\n\n# --- LangGraph ---\nfrom langgraph.graph import StateGraph, END, START\nfrom langgraph.graph.message import add_messages\nfrom langgraph.checkpoint.memory import MemorySaver # In-memory checkpointer\n\n# --- Gradio ---\nimport gradio as gr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"commands = [\n        \"curl -fsSL https://ollama.com/install.sh | sh\",\n]\nrun(commands)\n\nimport os\nos.system(\"/usr/local/bin/ollama serve &\")\nos.system(\"echo 'ollama test'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"commands = [\n        \"ollama pull nomic-embed-text\"\n]\nrun(commands)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pdf=PyPDFLoader(\"/kaggle/input/optopp/Book.pdf\")\nchunks=SemanticChunker(embeddings=embed)\npdf_documents = pdf.load()\n# Then split the loaded documents using the previously defined chunker\ndocs = chunks.split_documents(pdf_documents)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorstore = FAISS.from_documents(docs, embed)\nvectorstore.save_local(\"cadb\")\nvectorstore.as_retriever()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nsecret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\nsecret_value_2 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\nsecret_value_3 = user_secrets.get_secret(\"TAVILY_API_KEY\")\n\nos.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT', 'Business Law RAG Agent')\n\n# --- Global Variables & Configuration ---\nVECTOR_STORE_PATH = \"cadb\"\nOLLAMA_EMBED_MODEL = \"nomic-embed-text\" # Ollama model for embeddings\nGROQ_MODEL = 'meta-llama/llama-4-scout-17b-16e-instruct' # Groq model (ensure this is available)\nGEMINI_MODEL = \"gemini-2.0-flash\" # Google model\n\n# Global variable to hold the retriever once the PDF is processed\nglobal_retriever = None\nindexing_done = False\n\n\ndef load_retriever() -> Optional[Any]:\n    \"\"\"Loads the FAISS retriever from local storage.\"\"\"\n    global indexing_done\n    if not os.path.exists(VECTOR_STORE_PATH):\n         logger.warning(f\"FAISS index not found at {VECTOR_STORE_PATH}. Please upload and index a PDF.\")\n         indexing_done = False\n         return None\n    try:\n        logger.info(f\"Loading FAISS index from {VECTOR_STORE_PATH}...\")\n        embeddings = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL)\n        vectorstore = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)\n        logger.info(\"FAISS index loaded successfully.\")\n        indexing_done = True\n        return vectorstore.as_retriever()\n    except Exception as e:\n        logger.error(f\"Error loading FAISS index: {e}\", exc_info=True)\n        indexing_done = False\n        return None\n\n# --- Initialize LLMs and Tools ---\nlogger.info(\"Initializing LLMs and Tools...\")\ntry:\n    llm_fast = ChatGroq(model=GROQ_MODEL, temperature=0.2) # For simpler tasks like grading/rewriting\n    llm_gen = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.3) # For generation\n\n    # Attempt to load retriever on startup if index exists\n    global_retriever = load_retriever()\n\n    search_tool = TavilySearchResults(max_results=2, search_depth='basic')\n    logger.info(\"LLMs and Tavily Search Tool initialized.\")\n\nexcept Exception as e:\n    logger.error(f\"Initialization Error: {e}\", exc_info=True)\n    # Handle error appropriately - maybe exit or disable features\n    raise SystemExit(\"Failed to initialize core components.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- RAG Graph Definition ---\n\nclass RAGGraphState(TypedDict):\n    \"\"\"State for the RAG pipeline.\"\"\"\n    original_question: str\n    question: str\n    documents: List[Document]\n    is_rag_relevant: bool | None\n    generation: str\n    source_used: Literal[\"Vector Store\", \"Web Search\", \"None\"]\n\n# Pydantic model for the grader\nclass GradeDocuments(PydanticBaseModel):\n    \"\"\"Binary score for relevance check.\"\"\"\n    binary_score: Literal['yes', 'no'] = Field(description=\"Is the document relevant? ('yes' or 'no')\")\n\n# RAG Nodes (Adapted for Business Law Context)\ndef rewrite_query_node(state: RAGGraphState) -> RAGGraphState:\n    \"\"\"Rewrites the query for legal context retrieval.\"\"\"\n    logger.info(\"--- RAG Node: REWRITE QUERY ---\")\n    original_question = state['original_question']\n    rewrite_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a query rewriting expert specializing in Indian Business Law and the Companies Act. Rewrite the user's question to be precise and effective for retrieving relevant legal sections or explanations from a database. Focus on legal terms, section numbers (if mentioned), and core concepts. Do not answer the question, only rewrite.\"),\n        (\"human\", \"Original question: {question}\\n\\nRewritten query for legal database:\"),\n    ])\n    rewriter_chain = rewrite_prompt | llm_fast | StrOutputParser()\n    try:\n        rewritten_question = rewriter_chain.invoke({\"question\": original_question})\n        logger.info(f\"Rewritten Query: {rewritten_question}\")\n        return {\"question\": rewritten_question, \"original_question\": original_question}\n    except Exception as e:\n         logger.error(f\"Error rewriting query: {e}\", exc_info=True)\n         return {\"question\": original_question, \"original_question\": original_question} # Fallback\n\n\ndef retrieve_rag_node(state: RAGGraphState) -> RAGGraphState:\n    \"\"\"Retrieves documents using the global retriever.\"\"\"\n    logger.info(\"--- RAG Node: RETRIEVE (Vector Store) ---\")\n    if global_retriever is None:\n        logger.warning(\"Retriever not available. Skipping RAG retrieval.\")\n        return {\"documents\": [], \"source_used\": \"None\"}\n    question = state['question']\n    try:\n        logger.info(f\"Retrieving documents for: {question}\")\n        docs = global_retriever.invoke(question)\n        logger.info(f\"Retrieved {len(docs)} documents from vector store.\")\n        return {\"documents\": docs, \"source_used\": \"Vector Store\"}\n    except Exception as e:\n         logger.error(f\"Error during RAG retrieval: {e}\", exc_info=True)\n         return {\"documents\": [], \"source_used\": \"None\"}\n\ndef grade_documents_node(state: RAGGraphState) -> RAGGraphState:\n    \"\"\"Grades the relevance of retrieved documents.\"\"\"\n    logger.info(\"--- RAG Node: GRADE DOCUMENTS ---\")\n    question = state['question']\n    documents = state['documents']\n    if not documents:\n        logger.warning(\"No documents to grade.\")\n        return {\"is_rag_relevant\": False}\n\n    system = \"\"\"You are a grader assessing the relevance of a retrieved document snippet to a user question about Indian Business Law or the Companies Act.\n    Focus ONLY on relevance. Does the document contain keywords, legal concepts, section numbers, or context that could potentially help answer the question?\n    Output a binary score 'yes' or 'no' based SOLELY on relevance.\"\"\"\n    grade_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", system),\n        (\"human\", \"Document snippet:\\n\\n{document}\\n\\nUser question: {question}\"),\n    ])\n    structured_llm_grader = llm_fast.with_structured_output(GradeDocuments)\n    grader_chain = grade_prompt | structured_llm_grader\n\n    relevant_docs_found = False\n    for doc in documents[:2]: # Grade only first few for speed\n        try:\n            doc_snippet = doc.page_content[:1000] # Limit context for grader\n            score = grader_chain.invoke({\"question\": question, \"document\": doc_snippet})\n            if score.binary_score == 'yes':\n                logger.info(\"--- GRADE: Relevant document found (Vector Store) ---\")\n                relevant_docs_found = True\n                break\n            else:\n                logger.info(\"--- GRADE: Document deemed NOT relevant (Vector Store) ---\")\n        except Exception as e:\n            logger.error(f\"Error grading document: {e}\", exc_info=True)\n            # Treat grading errors as not relevant for robustness\n            continue\n\n    logger.info(f\"Vector Store Relevance Decision: {'Relevant' if relevant_docs_found else 'Not Relevant'}\")\n    return {\"is_rag_relevant\": relevant_docs_found}\n\ndef web_search_node(state: RAGGraphState) -> RAGGraphState:\n    \"\"\"Performs web search if RAG fails.\"\"\"\n    logger.info(\"--- RAG Node: WEB SEARCH ---\")\n    question = state['question'] # Use rewritten question for search\n    logger.info(f\"Performing web search for: {question}\")\n    docs = []\n    try:\n        web_results = search_tool.invoke({\"query\": question})\n        if web_results and isinstance(web_results, list):\n             docs = [Document(page_content=d.get(\"content\", \"\"), metadata={\"source\": d.get(\"url\", \"web\")})\n                    for d in web_results if d.get(\"content\")]\n             logger.info(f\"Found {len(docs)} web results.\")\n        else:\n             logger.warning(f\"Web search returned unexpected or empty results: {web_results}\")\n    except Exception as e:\n        logger.error(f\"Error during web search: {e}\", exc_info=True)\n        docs = [Document(page_content=\"Web search failed due to an error.\", metadata={\"source\": \"error\"})]\n\n    return {\"documents\": docs, \"source_used\": \"Web Search\"}\n\n\ndef generate_answer_node(state: RAGGraphState) -> RAGGraphState:\n    \"\"\"Generates the final answer using context.\"\"\"\n    logger.info(\"--- RAG Node: GENERATE ANSWER ---\")\n    question = state['original_question']\n    documents = state['documents']\n    source = state.get('source_used', 'Unknown') # Get source info\n\n    if not documents:\n        logger.warning(\"No documents available for generation.\")\n        return {\"generation\": \"I could not find relevant information from the provided document or web search to answer your question.\"}\n\n    context = \"\\n\\n\".join([f\"Source: {doc.metadata.get('source', source)}\\nContent: {doc.page_content}\" for doc in documents])\n\n    prompt_template = \"\"\"You are an expert assistant specializing in Indian Business Law, particularly concepts covered in the provided study materials or relevant web searches.\n\nCONTEXT INFORMATION:\n{context}\n\nUSER QUESTION: {question}\n\nINSTRUCTIONS:\n1. Provide an accurate, clear, and concise answer based **strictly** on the CONTEXT INFORMATION provided above.\n2. If the context comes from the \"Vector Store\" (the uploaded PDF), prioritize that information. If it comes from \"Web Search\", state that clearly (e.g., \"According to web search results...\").\n3. Explain legal concepts in simple terms suitable for a foundation-level CA student. Define technical terms briefly if necessary.\n4. If the context allows, mention relevant section numbers (e.g., \"As per Section X of the Indian Contract Act...\").\n5. **Crucially:** If the context does **not** contain the information to answer the question, state clearly: \"Based on the provided information, I cannot answer this question.\" Do **not** invent information or draw external knowledge not present in the context.\n6. Format the answer for readability using paragraphs or bullet points.\n\nAnswer:\n\"\"\"\n    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n    rag_chain = prompt | llm_gen | StrOutputParser() # Use the more capable LLM for generation\n\n    try:\n        generation = rag_chain.invoke({\"context\": context, \"question\": question})\n    except Exception as e:\n        logger.error(f\"Error during generation: {e}\", exc_info=True)\n        generation = \"Sorry, an error occurred while generating the response.\"\n\n    logger.info(f\"Generated Answer (Source: {source}): {generation[:150]}...\")\n    return {\"generation\": generation}\n\ndef decide_search_or_generate(state: RAGGraphState) -> Literal[\"web_search\", \"generate_answer\"]:\n    \"\"\"Decision node for the RAG pipeline.\"\"\"\n    logger.info(\"--- RAG Edge: Decide Search or Generate ---\")\n    if state.get('is_rag_relevant', False):\n        logger.info(\"Decision: Relevant docs found in vector store -> Generate Answer.\")\n        return \"generate_answer\"\n    else:\n        logger.info(\"Decision: No relevant docs in vector store -> Web Search.\")\n        return \"web_search\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logger.info(\"Building the RAG workflow graph...\")\nrag_workflow = StateGraph(RAGGraphState)\nrag_workflow.add_node(\"rewrite_query\", rewrite_query_node)\nrag_workflow.add_node(\"retrieve_rag\", retrieve_rag_node)\nrag_workflow.add_node(\"grade_documents\", grade_documents_node)\nrag_workflow.add_node(\"web_search\", web_search_node)\nrag_workflow.add_node(\"generate_answer\", generate_answer_node)\n\nrag_workflow.set_entry_point(\"rewrite_query\")\nrag_workflow.add_edge(\"rewrite_query\", \"retrieve_rag\")\nrag_workflow.add_edge(\"retrieve_rag\", \"grade_documents\")\nrag_workflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_search_or_generate,\n    {\"web_search\": \"web_search\", \"generate_answer\": \"generate_answer\"}\n)\nrag_workflow.add_edge(\"web_search\", \"generate_answer\")\nrag_workflow.add_edge(\"generate_answer\", END)\n\ntry:\n    rag_app = rag_workflow.compile()\n    logger.info(\"RAG workflow graph compiled successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error compiling RAG graph: {e}\", exc_info=True)\n    rag_app = None\n    raise SystemExit(\"Failed to compile RAG workflow.\")\n\n\n# --- Conversational Graph Definition ---\nclass ConversationState(TypedDict):\n    \"\"\"State for the conversational agent.\"\"\"\n    messages: Annotated[List[BaseMessage], add_messages]\n\n# Checkpointer for storing conversation history\nmemory = MemorySaver()\n\ndef call_rag_pipeline(state: ConversationState) -> ConversationState:\n    \"\"\"Node that invokes the RAG pipeline for the latest user query.\"\"\"\n    logger.info(\"--- Conversation Node: CALL RAG PIPELINE ---\")\n    last_message = state['messages'][-1]\n    if not isinstance(last_message, HumanMessage):\n        # Should not happen in typical flow, but good practice\n        logger.warning(\"Last message is not a HumanMessage, skipping RAG call.\")\n        return {\"messages\": [AIMessage(content=\"Internal error: Expected user input.\")]}\n\n    user_query = last_message.content\n    logger.info(f\"Invoking RAG pipeline for query: {user_query}\")\n\n    if rag_app is None:\n        logger.error(\"RAG App is not compiled. Cannot process query.\")\n        return {\"messages\": [AIMessage(content=\"Error: The document processing pipeline is not ready.\")]}\n    if not indexing_done and global_retriever is None:\n         logger.warning(\"PDF not indexed. RAG pipeline will rely on web search only.\")\n         # Proceed, but generation node will know source is web or none\n\n    rag_input = {\"original_question\": user_query}\n    try:\n        rag_output = rag_app.invoke(rag_input)\n        generated_answer = rag_output.get(\"generation\", \"Sorry, I couldn't retrieve an answer.\")\n        logger.info(\"RAG pipeline finished.\")\n        # The conversational LLM will now use this generated answer as context\n        # We just pass the AI message back into the conversation state\n        return {\"messages\": [AIMessage(content=generated_answer)]}\n    except Exception as e:\n        logger.error(f\"Error invoking RAG pipeline: {e}\", exc_info=True)\n        return {\"messages\": [AIMessage(content=\"Sorry, there was an error retrieving information.\")]}\n\n# Build the Conversational Graph\nlogger.info(\"Building the conversational workflow graph...\")\nconversation_graph = StateGraph(ConversationState)\nconversation_graph.add_node(\"call_rag\", call_rag_pipeline)\nconversation_graph.set_entry_point(\"call_rag\")\nconversation_graph.add_edge(\"call_rag\", END)\n\ntry:\n    # Compile with memory\n    agent_executor = conversation_graph.compile(checkpointer=memory)\n    logger.info(\"Conversational agent compiled successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error compiling conversational agent: {e}\", exc_info=True)\n    agent_executor = None\n    raise SystemExit(\"Failed to compile conversational agent.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def handle_pdf_upload(pdf_file):\n    \"\"\"Handles PDF upload, processing, and retriever setup.\"\"\"\n    global global_retriever, indexing_done\n    if pdf_file is None:\n        return \"Status: No PDF uploaded. Using existing index if available, or web search only.\", None # Return None for chatbot\n\n    file_path = pdf_file.name\n    logger.info(f\"Processing uploaded PDF: {file_path}\")\n    status_update = f\"Processing PDF: {os.path.basename(file_path)}...\"\n    yield status_update, None # Update status, clear chatbot\n\n    vectorstore = process_pdf(file_path)\n\n    if vectorstore:\n        global_retriever = vectorstore.as_retriever()\n        indexing_done = True\n        status_update = f\"Status: Successfully indexed {os.path.basename(file_path)}. Ready to chat.\"\n        logger.info(\"Retriever updated with new PDF.\")\n        yield status_update, None\n    else:\n        status_update = f\"Status: Failed to index {os.path.basename(file_path)}. Please check logs. Will rely on web search.\"\n        global_retriever = None # Ensure retriever is None if indexing failed\n        indexing_done = False\n        logger.error(\"Retriever could not be created from PDF.\")\n        yield status_update, None\n\n\ndef chat_interface(message, history, thread_id_state):\n    \"\"\"Handles the chat interaction with the LangGraph agent.\"\"\"\n    logger.info(f\"Received message for thread_id {thread_id_state}: {message}\")\n\n    if agent_executor is None:\n         logger.error(\"Agent executor not available.\")\n         # This error should ideally be shown more prominently in the UI\n         history.append((message, \"Error: Chat agent is not ready.\"))\n         return \"\", history, thread_id_state\n\n    if not indexing_done and global_retriever is None:\n         logger.warning(\"Chatting without an indexed PDF. Relying on web search.\")\n         # Optionally add a notice to the user in the chat history here\n\n    # LangGraph configuration\n    config = {\"configurable\": {\"thread_id\": thread_id_state}}\n\n    # Append user message to Gradio history immediately\n    # history.append((message, None)) # This causes duplicate display, manage history return instead\n    # yield \"\", history, thread_id_state # Update UI to show user message\n\n    # Stream the response\n    response_content = \"\"\n    try:\n        logger.info(f\"Streaming response for thread_id {thread_id_state}\")\n        # Use stream for potentially better UX, fallback to invoke if needed\n        # Note: Streaming directly into Gradio chatbot needs careful handling\n        # For simplicity, let's use invoke and update history at the end\n\n        # inputs = {\"messages\": [HumanMessage(content=message)]} # add_messages handles history\n        inputs = {\"messages\": HumanMessage(content=message)} # Correct way with add_messages\n\n        # Invoke the agent\n        result = agent_executor.invoke(inputs, config)\n\n        # Get the last AI message\n        if result and \"messages\" in result and len(result[\"messages\"]) > 0:\n             # The result['messages'] contains the whole history now due to add_messages\n             # The last one should be the AI's response to the current input\n            ai_msg_obj = result[\"messages\"][-1]\n            if isinstance(ai_msg_obj, AIMessage):\n                 response_content = ai_msg_obj.content\n            else:\n                 response_content = \"Error: Agent returned an unexpected message type.\"\n                 logger.error(f\"Unexpected message type from agent: {type(ai_msg_obj)}\")\n        else:\n            response_content = \"Sorry, I couldn't process that.\"\n            logger.error(f\"Agent did not return expected messages. Result: {result}\")\n\n        logger.info(f\"Agent response received for thread_id {thread_id_state}\")\n\n    except Exception as e:\n        logger.error(f\"Error during agent invocation for thread_id {thread_id_state}: {e}\", exc_info=True)\n        response_content = f\"An error occurred: {e}\"\n\n    # Update Gradio history with the AI response\n    # Convert LangChain message history to Gradio format\n    gradio_history = []\n    if result and \"messages\" in result:\n        lc_messages = result[\"messages\"]\n        # Iterate through pairs (or handle the first message if odd)\n        for i in range(0, len(lc_messages), 2):\n            user_msg = lc_messages[i].content if isinstance(lc_messages[i], HumanMessage) else None\n            ai_msg = lc_messages[i+1].content if (i+1 < len(lc_messages)) and isinstance(lc_messages[i+1], AIMessage) else None\n            if user_msg is not None: # Should always have user message first in pairs\n                 gradio_history.append((user_msg, ai_msg))\n        # Handle case where the last message is the user's (shouldn't happen after invoke)\n        # or if history started with AI (unlikely with add_messages)\n    else: # Fallback if invoke failed badly\n         history.append((message, response_content))\n         gradio_history = history\n\n\n    # Return empty string for the textbox, updated history, and the same thread_id\n    return \"\", gradio_history, thread_id_state\n\n\n# --- Build Gradio App ---\nlogger.info(\"Building Gradio interface...\")\nwith gr.Blocks(theme=gr.themes.Soft(), title=\"CA Business Law Chat\") as demo:\n    gr.Markdown(\"# Chat with CA Foundation Business Law Assistant\")\n    gr.Markdown(\"Upload your CA Foundation Business Law PDF (Syllabus 2022 context provided in the example) and ask questions.\")\n\n    # Hidden state for conversation thread ID\n    thread_id_state = gr.State(value=str(uuid.uuid4())) # Initialize with a new ID\n\n    with gr.Row():\n        with gr.Column(scale=1):\n            pdf_upload = gr.File(label=\"Upload Business Law PDF\", file_types=[\".pdf\"])\n            index_button = gr.Button(\"Index PDF\")\n            upload_status = gr.Textbox(\"Status: Please upload and index PDF.\", label=\"Indexing Status\", interactive=False)\n\n        with gr.Column(scale=4):\n            chatbot = gr.Chatbot(label=\"Chat History\", height=600)\n            msg_textbox = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about Contracts Act, Sale of Goods Act, etc.\", scale=7)\n            submit_button = gr.Button(\"Send\", variant=\"primary\", scale=1)\n            clear_button = gr.ClearButton([msg_textbox, chatbot], value=\"Clear Chat\")\n\n    # Event Handlers\n    upload_event = index_button.click(\n        fn=handle_pdf_upload,\n        inputs=[pdf_upload],\n        outputs=[upload_status, chatbot] # Update status and clear chat on new index\n    )\n\n    # Handle message submission (using Enter key or Send button)\n    msg_textbox.submit(\n         fn=chat_interface,\n         inputs=[msg_textbox, chatbot, thread_id_state],\n         outputs=[msg_textbox, chatbot, thread_id_state] # Textbox cleared, chatbot updated\n    )\n    submit_button.click(\n         fn=chat_interface,\n         inputs=[msg_textbox, chatbot, thread_id_state],\n         outputs=[msg_textbox, chatbot, thread_id_state]\n    )\n\n    # Handle clearing chat (doesn't reset thread_id implicitly here, might need adjustment if full reset is desired)\n    # ClearButton handles clearing components. If thread reset is needed, add custom logic.\n\n\nlogger.info(\"Gradio interface defined.\")\n\n# --- Launch the App ---\nif __name__ == \"__main__\":\n    logger.info(\"Launching Gradio App...\")\n    demo.launch(share=False) # share=True to create public link if needed (use with caution)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}